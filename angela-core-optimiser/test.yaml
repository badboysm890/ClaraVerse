# Auto-generated llama-swap configuration
# Models directory: C:\Users\prave\.angela\llama-models
healthCheckTimeout: 300
logLevel: info

models:
  "gpt-oss-20b-q5-k-m:20b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\gpt-oss-20b-Q5_K_M.gguf"
      --port 9999 --jinja --no-warmup --n-gpu-layers 9999 --threads 21 --ctx-size 32768 --batch-size 512 --ubatch-size 256 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "jan-nano-128k-q8-0":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\Jan-nano-128k-Q8_0.gguf"
      --port 9999 --jinja --no-warmup --n-gpu-layers 9999 --threads 21 --ctx-size 32768 --batch-size 512 --ubatch-size 256 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "jan-v1-4b-q8-0:4b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\Jan-v1-4B.Q8_0.gguf"
      --port 9999 --jinja --no-warmup --n-gpu-layers 9999 --threads 21 --ctx-size 32768 --batch-size 512 --ubatch-size 256 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "mxbai-embed-large:embed-iq4_xs":
    proxy: "http://127.0.0.1:9998"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\mxbai-large.IQ4_XS.gguf"
      --port 9998 --jinja --no-warmup --n-gpu-layers 9999 --pooling mean --embeddings --threads 21 --batch-size 512 --ubatch-size 256 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "qwen3:0.6b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\Qwen3-0.6B-Q8_0.gguf"
      --port 9999 --jinja --no-warmup --n-gpu-layers 9999 --threads 21 --ctx-size 32768 --batch-size 512 --ubatch-size 256 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "qwen3:4b-q8_0":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\Qwen3-4B-Thinking-2507-Q8_0.gguf"
      --port 9999 --jinja --no-warmup --n-gpu-layers 9999 --threads 21 --ctx-size 32768 --batch-size 512 --ubatch-size 256 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "qwen3:4b-q4_k_m":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\Qwen3-4B.Q4_K_M.gguf"
      --port 9999 --jinja --no-warmup --n-gpu-layers 9999 --threads 21 --ctx-size 32768 --batch-size 512 --ubatch-size 256 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "mxbai-embed-large:embed-q4_k_s":
    proxy: "http://127.0.0.1:9998"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\embedding\mxbai-embed-large-v1.Q4_K_S.gguf"
      --port 9998 --jinja --no-warmup --n-gpu-layers 9999 --pooling mean --embeddings --threads 21 --batch-size 512 --ubatch-size 256 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "gemma3:27b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\Gemma\gemma-3-27b-tools.Q4_K_M.gguf"
      --port 9999 --jinja --no-warmup --n-gpu-layers 9999
      --mmproj "C:\BackUP\llama-modelsss\Gemma\gemma-3-27b-tools.mmproj-f16.gguf" --threads 21 --ctx-size 10409 --batch-size 512 --ubatch-size 256 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "gemma3:4b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\Gemma\gemma-3-4b-it-Q4_K_S.gguf"
      --port 9999 --jinja --no-warmup --n-gpu-layers 9999 --threads 21 --ctx-size 32768 --batch-size 512 --ubatch-size 256 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "glm-4-5-air-iq4-xs-00001-of-00002":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\GLM-4.5-Air-IQ4_XS\GLM-4.5-Air-IQ4_XS-00001-of-00002.gguf"
      --port 9999 --jinja --no-warmup --n-gpu-layers 22 --threads 21 --ctx-size 8192 --batch-size 512 --ubatch-size 128 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

  "qwen3:30b":
    proxy: "http://127.0.0.1:9999"
    cmd: |
      "C:\Users\prave\AppData\Local\Programs\angela\resources\electron\llamacpp-binaries\win32-x64-cuda\llama-server.exe"
      -m "C:\BackUP\llama-modelsss\Qwen\Qwen_Qwen3-30B-A3B-Instruct-2507-Q5_K_M.gguf"
      --port 9999 --jinja --no-warmup --n-gpu-layers 48 --threads 21 --ctx-size 14143 --batch-size 512 --ubatch-size 128 --keep 1024 --defrag-thold 0.1 --parallel 1 --flash-attn --cont-batching
    ttl: 300

groups:
  "embedding_models":
    # Allow multiple embedding models to run together
    swap: false
    # Don't unload other groups when embedding models start
    exclusive: false
    # Prevent other groups from unloading embedding models
    persistent: true
    members:
      - "mxbai-embed-large:embed-iq4_xs"
      - "mxbai-embed-large:embed-q4_k_s"

  "regular_models":
    # Only one regular model at a time (traditional behavior)
    swap: true
    # Unload other non-persistent groups when loading
    exclusive: true
    members:
      - "gpt-oss-20b-q5-k-m:20b"
      - "jan-nano-128k-q8-0"
      - "jan-v1-4b-q8-0:4b"
      - "qwen3:0.6b"
      - "qwen3:4b-q8_0"
      - "qwen3:4b-q4_k_m"
      - "gemma3:27b"
      - "gemma3:4b"
      - "glm-4-5-air-iq4-xs-00001-of-00002"
      - "qwen3:30b"
